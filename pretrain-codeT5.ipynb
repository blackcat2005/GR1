{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":57023,"sourceType":"modelInstanceVersion","modelInstanceId":47840}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Pretrain","metadata":{}},{"cell_type":"code","source":"DATAPATH = \"/kaggle/input/codesearchnet/ruby/ruby/final/jsonl\"","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n!pip install transformers \nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nimport torch.nn as nn \nimport torch.nn.functional as F\nfrom torch.optim import AdamW\n!pip install contractions\nimport contractions\nimport os\nimport pyarrow.parquet as pq\nimport re\nimport time\nimport gc\nfrom tqdm.notebook import tqdm\nfrom itertools import filterfalse\nfrom tqdm import trange\nfrom transformers import AutoTokenizer, BertForPreTraining,BertForMaskedLM\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, RobertaTokenizer\nimport torch\nimport random","metadata":{"execution":{"iopub.status.busy":"2024-05-28T13:55:25.221661Z","iopub.execute_input":"2024-05-28T13:55:25.222145Z","iopub.status.idle":"2024-05-28T13:56:10.594132Z","shell.execute_reply.started":"2024-05-28T13:55:25.222107Z","shell.execute_reply":"2024-05-28T13:56:10.592675Z"},"_kg_hide-input":true,"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: contractions in /opt/conda/lib/python3.10/site-packages (0.1.73)\nRequirement already satisfied: textsearch>=0.0.21 in /opt/conda/lib/python3.10/site-packages (from contractions) (0.0.24)\nRequirement already satisfied: anyascii in /opt/conda/lib/python3.10/site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\nRequirement already satisfied: pyahocorasick in /opt/conda/lib/python3.10/site-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"def sliding_window(row, col_name, chunk_size=509, overlap=50):\n    words = row[col_name]\n    chunks = []\n    for i in range(0, len(words), chunk_size - overlap):\n        start = i\n        end = min(i + chunk_size, len(words))\n        chunk = ' '.join(words[start:end])\n        chunks.append(chunk)\n    return pd.DataFrame({'docstring': chunks})\ndef expand_contractions(sentence):\n    contractions_expanded = [contractions.fix(word) for word in sentence.split()]\n    return ' '.join(contractions_expanded)\ndef lower_case(sentence):\n    return ' '.join([word.lower() for word in sentence.split()])\ndef remove_punctuation(sentence):\n    return ' '.join([re.sub(r'[^\\w\\s]', '', word) for word in sentence.split()])\ndef preprocess(lst, process=True, min_words=20):\n    lst[:] = filterfalse(lambda x: len(x.split()) <= min_words, lst)\n    if process == True:\n        for i, sent in enumerate(lst):\n            # if len(sent.split()) <= min_words:\n            #   continue\n            lst[i] = lower_case(remove_punctuation(expand_contractions(sent)))\n    return lst\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed(42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_path = DATAPATH + '/train'\njson_files = [f for f in os.listdir(train_path) if f.endswith('.jsonl')]\ndataframes = []\n\nfor file in json_files:\n    file_path = os.path.join(train_path, file)\n    df = pd.read_json(file_path, lines=True)\n    dataframes.append(df)\n\ndf_self_sup = pd.concat(dataframes, ignore_index=True)\ndf_self_sup = pd.concat([sliding_window(row, 'docstring_tokens') for _, row in df_self_sup.iterrows()], ignore_index=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nset_seed(42)\nstart_time = time.time()\ntrain_sentences = preprocess(list(df_self_sup['docstring']), min_words = 50)\ntrain_df = pd.DataFrame([])\ntrain_df['docstring'] = train_sentences\ndel train_sentences, df_self_sup\ngc.collect()\n# Load the pre-trained T5 model and tokenizer\ntokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-small')\nmodel = T5ForConditionalGeneration.from_pretrained(\"Salesforce/codet5-small\").to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MaskedLanguageModelingDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, mask_probability=0.15, max_length=512):\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.mask_probability = mask_probability\n        self.max_length = max_length\n        self.special_tokens = self.tokenizer.additional_special_tokens \n    def mask_tokens(self, text):\n        tokens = text.split()\n        masked_tokens = []\n        idx = 0\n        i = 0\n        while i < len(tokens):\n            if random.random() < self.mask_probability:\n                # Replace all consecutive masked words with a single special token\n                current_special_token = self.special_tokens[idx % len(self.special_tokens)]\n                masked_tokens.append(current_special_token)\n                while i + 1 < len(tokens) and random.random() < self.mask_probability:\n                    i += 1\n            else:\n                masked_tokens.append(tokens[i])\n            i += 1\n            idx += 1\n\n        masked_text = \" \".join(masked_tokens)\n        return masked_text\n\n    def complement_tokens(self, text, masked_indices):\n        tokens = text.split()\n        complement_tokens = []\n        idx = 0\n        i = 0\n        while i < len(tokens):\n            if i in masked_indices:\n                # Replace all consecutive masked words with a single special token\n                current_special_token = self.special_tokens[idx % len(self.special_tokens)]\n                complement_tokens.append(current_special_token)\n                while i + 1 < len(tokens) and i + 1 in masked_indices:\n                    i += 1\n            else:\n                complement_tokens.append(tokens[i])\n            i += 1\n            idx += 1\n\n        complement_text = \" \".join(complement_tokens)\n        return complement_text\n    \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        text = self.data.iloc[idx]['docstring']\n\n        # Mask tokens with the given probability\n        masked_text = self.mask_tokens(text)\n\n        # Get the indices of masked tokens\n        masked_indices = [i for i, token in enumerate(masked_text.split()) if not token.startswith(\"<extra_id_\")]\n\n        # Create complement sentence\n        complement_text = self.complement_tokens(text, masked_indices)\n  \n        # Tokenize the masked text\n        input_ids = self.tokenizer(\n            masked_text,\n            return_tensors=\"pt\",\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True\n        ).input_ids\n\n        # Tokenize the complement text\n        labels = self.tokenizer(\n            complement_text,\n            return_tensors=\"pt\",\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True\n        ).input_ids\n\n        return {\"input_ids\": input_ids, \"labels\": labels}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = MaskedLanguageModelingDataset(train_df, tokenizer)\ndataloader = DataLoader(dataset, batch_size=8, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 5\nlearning_rate = 5e-5\n\n# Define optimizer\noptimizer = AdamW(model.parameters(), lr=learning_rate)\n\nnum_accumulation_steps = 32\n# Training loop\nfor epoch in range(epochs):\n    loop = tqdm(dataloader, leave=True)\n    i = 0\n    for batch in loop:\n        inputs = batch[\"input_ids\"].squeeze(dim=1).to(device)\n        labels = batch[\"labels\"].squeeze(dim=1).to(device)\n        outputs = model(input_ids=inputs, labels=labels)\n        loss = outputs.loss/num_accumulation_steps\n        loss.backward()\n        if (i+1) % num_accumulation_steps == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n            loop.set_description(f'Epoch {epoch}')\n            loop.set_postfix(loss=loss.item())\n        i+=1\n# Save the trained model\nmodel.save_pretrained(\"codet5_model_base\")\ntokenizer.save_pretrained(\"codet5_model_tokenizer_base\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Eval","metadata":{}},{"cell_type":"code","source":"tokenizer = T5Tokenizer.from_pretrained('/kaggle/working/unsupervised_t5_model_tokenizer_base')\nmodel = T5ForConditionalGeneration.from_pretrained('/kaggle/working/unsupervised_t5_model_base').to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_path = DATAPATH + '/test'\ntest_files = [f for f in os.listdir(test_path) if f.endswith('.jsonl')]\ndataframes = []\n\nfor file in test_files:\n    file_path = os.path.join(test_path, file)\n    df = pd.read_json(file_path, lines=True)\n    dataframes.append(df)\n\ndf_test = pd.concat(dataframes, ignore_index=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nclass JsonlDataset(Dataset):\n    def __init__(self, file_path):\n        self.data = []\n        with open(file_path, 'r') as f:\n            for line in f:\n                self.data.append(json.loads(line))\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        return self.data[index]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def postprocessing(text):\n    lines = text.splitlines()\n    filtered_lines = [line for line in lines if not ('@param' in line or '@return' in line)]\n    filtered_text = ' '.join(filtered_lines).replace('\\n', ' ').replace('\\t', ' ').replace('\\n\\n', ' ').replace('\\t\\t', ' ')\n    return filtered_text","metadata":{"execution":{"iopub.status.busy":"2024-05-28T09:12:47.946812Z","iopub.execute_input":"2024-05-28T09:12:47.947589Z","iopub.status.idle":"2024-05-28T09:12:47.953842Z","shell.execute_reply.started":"2024-05-28T09:12:47.947545Z","shell.execute_reply":"2024-05-28T09:12:47.952499Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# test_example = JsonlDataset('/kaggle/input/codesearchnet/ruby/ruby/final/jsonl/test/ruby_test_0.jsonl')\ntest_example = dataset['test']\nprint(test_example)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T08:45:21.339338Z","iopub.execute_input":"2024-05-28T08:45:21.340182Z","iopub.status.idle":"2024-05-28T08:45:21.347291Z","shell.execute_reply.started":"2024-05-28T08:45:21.340150Z","shell.execute_reply":"2024-05-28T08:45:21.346062Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['id', 'repo', 'path', 'func_name', 'original_string', 'language', 'code', 'code_tokens', 'docstring', 'docstring_tokens', 'sha', 'url', 'input_ids', 'attention_mask', 'labels'],\n    num_rows: 1261\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"code_x_glue_ct_code_to_text\", \"ruby\")\ntest_example = dataset['test']\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nwith open('/kaggle/working/eval/pretrain_t5/predictions.txt', 'w', encoding=\"utf8\") as pre, open('/kaggle/working/eval/pretrain_t5/reference.txt', 'w', encoding=\"utf8\") as re:\n    testloader = DataLoader(test_example, batch_size=1, shuffle=False)\n    length = len(dataloader)\n    i = 0\n    print(test_example)\n    for batch in testloader:\n        # Prediction\n        text = batch['code'][0]\n        input_ids = tokenizer(text, return_tensors=\"pt\", max_length=17500, truncation=True, padding=True).input_ids\n        input_ids = input_ids.squeeze(dim=1).to(device)\n        generated_ids = model.generate(input_ids, max_length=20)\n        result = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n        result = postprocessing(result)\n        pre.write(str(i) + '\\t' + result + \"\\n\")\n\n        # Reference\n        text = batch['docstring']\n        text= postprocessing(text)\n        re.write(str(i) + '\\t' + text + \"\\n\")\n\n        i += 1\n        if ((i % 100) == 0):\n          print(f'{i}/{length}')","metadata":{"execution":{"iopub.status.busy":"2024-05-28T08:55:01.147547Z","iopub.execute_input":"2024-05-28T08:55:01.148433Z","iopub.status.idle":"2024-05-28T08:55:16.087023Z","shell.execute_reply.started":"2024-05-28T08:55:01.148396Z","shell.execute_reply":"2024-05-28T08:55:16.085490Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['id', 'repo', 'path', 'func_name', 'original_string', 'language', 'code', 'code_tokens', 'docstring', 'docstring_tokens', 'sha', 'url'],\n    num_rows: 1261\n})\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[24], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m17500\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39minput_ids\n\u001b[1;32m     16\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 17\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m(input_ids, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m     18\u001b[0m result \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(generated_ids[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m result \u001b[38;5;241m=\u001b[39m postprocessing(result)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mAttributeError\u001b[0m: 'CodeT5' object has no attribute 'generate'"],"ename":"AttributeError","evalue":"'CodeT5' object has no attribute 'generate'","output_type":"error"}]},{"cell_type":"markdown","source":"# Finetune","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers datasets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q pytorch-lightning wandb","metadata":{"execution":{"iopub.status.busy":"2024-05-28T07:34:43.867538Z","iopub.execute_input":"2024-05-28T07:34:43.867918Z","iopub.status.idle":"2024-05-28T07:34:58.283145Z","shell.execute_reply.started":"2024-05-28T07:34:43.867885Z","shell.execute_reply":"2024-05-28T07:34:58.281968Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"code_x_glue_ct_code_to_text\", \"ruby\")\nprint(dataset)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T09:39:01.802459Z","iopub.execute_input":"2024-05-28T09:39:01.803496Z","iopub.status.idle":"2024-05-28T09:39:11.711742Z","shell.execute_reply.started":"2024-05-28T09:39:01.803450Z","shell.execute_reply":"2024-05-28T09:39:11.710229Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'repo', 'path', 'func_name', 'original_string', 'language', 'code', 'code_tokens', 'docstring', 'docstring_tokens', 'sha', 'url'],\n        num_rows: 24927\n    })\n    validation: Dataset({\n        features: ['id', 'repo', 'path', 'func_name', 'original_string', 'language', 'code', 'code_tokens', 'docstring', 'docstring_tokens', 'sha', 'url'],\n        num_rows: 1400\n    })\n    test: Dataset({\n        features: ['id', 'repo', 'path', 'func_name', 'original_string', 'language', 'code', 'code_tokens', 'docstring', 'docstring_tokens', 'sha', 'url'],\n        num_rows: 1261\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"example = dataset['train'][0]\n\nprint('Code: ' + example['code'])\nprint('Docstring ' + example['docstring'])","metadata":{"execution":{"iopub.status.busy":"2024-05-28T07:35:29.740691Z","iopub.execute_input":"2024-05-28T07:35:29.741128Z","iopub.status.idle":"2024-05-28T07:35:29.748717Z","shell.execute_reply.started":"2024-05-28T07:35:29.741102Z","shell.execute_reply":"2024-05-28T07:35:29.747678Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Code: def handle_parsed_websocket_message(json_data)\n      data =  json_data.is_a?(Hash) ? json_data.stringify_keys : {}\n      if CelluloidPubsub::Reactor::AVAILABLE_ACTIONS.include?(data['client_action'].to_s)\n        log_debug \"#{self.class} finds actions for  #{json_data}\"\n        delegate_action(data) if data['client_action'].present?\n      else\n        handle_unknown_action(data['channel'], json_data)\n      end\n    end\nDocstring method that checks if the data is a Hash\n\n if the data is a hash then will stringify the keys and will call the method {#delegate_action}\n that will handle the message, otherwise will call the method {#handle_unknown_action}\n\n @see #delegate_action\n @see #handle_unknown_action\n\n @param [Hash] json_data\n\n @return [void]\n\n @api public\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import RobertaTokenizer\n\ntokenizer = RobertaTokenizer.from_pretrained(\"Salesforce/codet5-small\")\n\nprefix = \"Summarize Ruby: \"\nmax_input_length = 1024\nmax_target_length = 1024\n\ndef preprocess_examples(examples):\n  # encode the code-docstring pairs\n  codes = examples['code']\n  docstrings = examples['docstring']\n\n  inputs = [prefix + code for code in codes]\n  model_inputs = tokenizer(inputs, max_length=max_input_length, padding=\"max_length\", truncation=True)\n\n  # encode the summaries\n  labels = tokenizer(docstrings, max_length=max_target_length, padding=\"max_length\", truncation=True).input_ids\n\n  # important: we need to replace the index of the padding tokens by -100\n  # such that they are not taken into account by the CrossEntropyLoss\n  labels_with_ignore_index = []\n  for labels_example in labels:\n    labels_example = [label if label != 0 else -100 for label in labels_example]\n    labels_with_ignore_index.append(labels_example)\n\n  model_inputs[\"labels\"] = labels_with_ignore_index\n\n  return model_inputs","metadata":{"execution":{"iopub.status.busy":"2024-05-28T07:35:29.750094Z","iopub.execute_input":"2024-05-28T07:35:29.750426Z","iopub.status.idle":"2024-05-28T07:35:39.298132Z","shell.execute_reply.started":"2024-05-28T07:35:29.750401Z","shell.execute_reply":"2024-05-28T07:35:39.297347Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.48k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"388d5a21caed400f8ae58b829768eb33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/703k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e14757458224d6ea1592929e4317c18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/294k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c5116e7967d4df2b0161fce1d161ddc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a86615d7872242048bb0cc6995b8564b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/12.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fae4820d6c56426b8ac79c6c6651c161"}},"metadata":{}}]},{"cell_type":"code","source":"dataset = dataset.map(preprocess_examples, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T07:35:39.300409Z","iopub.execute_input":"2024-05-28T07:35:39.300879Z","iopub.status.idle":"2024-05-28T07:36:52.365276Z","shell.execute_reply.started":"2024-05-28T07:35:39.300851Z","shell.execute_reply":"2024-05-28T07:36:52.364230Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/24927 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"075fa0414f044f36a8292739bf414daf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1438594166b435c9d765160f370ec43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1261 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53cf78829bd2480ca1335041f4ad5202"}},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndataset.set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'labels'])\ntrain_dataloader = DataLoader(dataset['train'], shuffle=True, batch_size=8)\nvalid_dataloader = DataLoader(dataset['validation'], batch_size=4)\ntest_dataloader = DataLoader(dataset['test'], batch_size=4)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T07:36:52.366331Z","iopub.execute_input":"2024-05-28T07:36:52.366594Z","iopub.status.idle":"2024-05-28T07:36:52.375249Z","shell.execute_reply.started":"2024-05-28T07:36:52.366571Z","shell.execute_reply":"2024-05-28T07:36:52.374249Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from transformers import T5ForConditionalGeneration, AdamW, get_linear_schedule_with_warmup\nimport pytorch_lightning as pl\n\nclass CodeT5(pl.LightningModule):\n    def __init__(self, lr=5e-5, num_train_epochs=2, warmup_steps=1000):\n        super().__init__()\n        self.model = T5ForConditionalGeneration.from_pretrained(\"/kaggle/input/pretraincodet5/pytorch/model/1\")\n        self.save_hyperparameters()\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        return outputs\n\n    def common_step(self, batch, batch_idx):\n        outputs = self(**batch)\n        loss = outputs.loss\n\n        return loss\n\n    def training_step(self, batch, batch_idx):\n        loss = self.common_step(batch, batch_idx)\n        # logs metrics for each training_step,\n        # and the average across the epoch\n        self.log(\"training_loss\", loss)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        loss = self.common_step(batch, batch_idx)\n        self.log(\"validation_loss\", loss, on_epoch=True)\n\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        loss = self.common_step(batch, batch_idx)\n\n        return loss\n\n    def configure_optimizers(self):\n        # create optimizer\n        optimizer = AdamW(self.parameters(), lr=self.hparams.lr)\n        # create learning rate scheduler\n        num_train_optimization_steps = self.hparams.num_train_epochs * len(train_dataloader)\n        lr_scheduler = {'scheduler': get_linear_schedule_with_warmup(optimizer,\n                                                    num_warmup_steps=self.hparams.warmup_steps,\n                                                    num_training_steps=num_train_optimization_steps),\n                        'name': 'learning_rate',\n                        'interval':'step',\n                        'frequency': 1}\n\n        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler}\n\n    def train_dataloader(self):\n        return train_dataloader\n\n    def val_dataloader(self):\n        return valid_dataloader\n\n    def test_dataloader(self):\n        return test_dataloader","metadata":{"execution":{"iopub.status.busy":"2024-05-28T07:36:52.376767Z","iopub.execute_input":"2024-05-28T07:36:52.377167Z","iopub.status.idle":"2024-05-28T07:36:55.694573Z","shell.execute_reply.started":"2024-05-28T07:36:52.377134Z","shell.execute_reply":"2024-05-28T07:36:55.693360Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import wandb\n\nwandb.login()","metadata":{"execution":{"iopub.status.busy":"2024-05-28T07:36:55.695928Z","iopub.execute_input":"2024-05-28T07:36:55.696571Z","iopub.status.idle":"2024-05-28T07:37:52.850142Z","shell.execute_reply.started":"2024-05-28T07:36:55.696535Z","shell.execute_reply":"2024-05-28T07:37:52.849128Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"model = CodeT5()","metadata":{"execution":{"iopub.status.busy":"2024-05-28T07:37:52.852119Z","iopub.execute_input":"2024-05-28T07:37:52.853199Z","iopub.status.idle":"2024-05-28T07:37:54.936325Z","shell.execute_reply.started":"2024-05-28T07:37:52.853170Z","shell.execute_reply":"2024-05-28T07:37:54.935331Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from pytorch_lightning import Trainer\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n\nwandb.finish()\nwandb_logger = WandbLogger(name='codet5-finetune-code-summarization', project='CodeT5')\n# for early stopping, see https://pytorch-lightning.readthedocs.io/en/1.0.0/early_stopping.html?highlight=early%20stopping\nearly_stop_callback = EarlyStopping(\n    monitor='validation_loss',\n    patience=3,\n    strict=False,\n    verbose=False,\n    mode='min'\n)\nlr_monitor = LearningRateMonitor(logging_interval='step')\n\ntrainer = Trainer(devices=1,\n                  default_root_dir=\"/kaggle/working/\",\n                  logger=wandb_logger,\n                  callbacks=[early_stop_callback, lr_monitor],\n                 max_epochs=3)\ntrainer.fit(model)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T07:37:54.939872Z","iopub.execute_input":"2024-05-28T07:37:54.940174Z","iopub.status.idle":"2024-05-28T08:22:32.097554Z","shell.execute_reply.started":"2024-05-28T07:37:54.940147Z","shell.execute_reply":"2024-05-28T08:22:32.096377Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mngaytanthe3579\u001b[0m (\u001b[33mhieund20052003\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>./wandb/run-20240528_073755-o4u7cmx0</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/hieund20052003/CodeT5/runs/o4u7cmx0' target=\"_blank\">codet5-finetune-code-summarization</a></strong> to <a href='https://wandb.ai/hieund20052003/CodeT5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/hieund20052003/CodeT5' target=\"_blank\">https://wandb.ai/hieund20052003/CodeT5</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/hieund20052003/CodeT5/runs/o4u7cmx0' target=\"_blank\">https://wandb.ai/hieund20052003/CodeT5/runs/o4u7cmx0</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f45134ccfa8e4faa9a1ed4957c3ad2c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-28T13:56:56.918663Z","iopub.execute_input":"2024-05-28T13:56:56.919080Z","iopub.status.idle":"2024-05-28T13:56:57.710483Z","shell.execute_reply.started":"2024-05-28T13:56:56.919048Z","shell.execute_reply":"2024-05-28T13:56:57.709132Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"dataset = load_dataset(\"code_x_glue_ct_code_to_text\", \"ruby\")\ntest_example = dataset['test']","metadata":{"execution":{"iopub.status.busy":"2024-05-28T13:56:59.936872Z","iopub.execute_input":"2024-05-28T13:56:59.938000Z","iopub.status.idle":"2024-05-28T13:57:07.385751Z","shell.execute_reply.started":"2024-05-28T13:56:59.937959Z","shell.execute_reply":"2024-05-28T13:57:07.384699Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"finetune_model = T5ForConditionalGeneration.from_pretrained(\"codet5_finetune\")\ntokenizer = RobertaTokenizer.from_pretrained(\"Salesforce/codet5-small\")","metadata":{"execution":{"iopub.status.busy":"2024-05-28T13:57:23.301154Z","iopub.execute_input":"2024-05-28T13:57:23.301661Z","iopub.status.idle":"2024-05-28T13:57:25.815252Z","shell.execute_reply.started":"2024-05-28T13:57:23.301614Z","shell.execute_reply":"2024-05-28T13:57:25.813890Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# prepare for the model\ninput_ids = tokenizer(test_example['code'], return_tensors='pt', max_length=512, truncation=True, padding=True).input_ids\n# generate\noutputs = finetune_model.generate(input_ids)\nprint(\"Generated docstring:\", tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2024-05-28T13:58:01.102369Z","iopub.execute_input":"2024-05-28T13:58:01.102967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def postprocessing(text):\n    lines = text\n    filtered_lines = [line for line in lines if not ('@param' in line or '@return' in line)]\n    filtered_text = ' '.join(filtered_lines).replace('\\n', ' ').replace('\\t', ' ').replace('\\n\\n', ' ').replace('\\t\\t', ' ')\n    return filtered_text","metadata":{"execution":{"iopub.status.busy":"2024-05-28T09:30:18.222551Z","iopub.execute_input":"2024-05-28T09:30:18.223017Z","iopub.status.idle":"2024-05-28T09:30:18.229679Z","shell.execute_reply.started":"2024-05-28T09:30:18.222963Z","shell.execute_reply":"2024-05-28T09:30:18.228501Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nwith open('/kaggle/working/eval/pretrain_t5/predictions.txt', 'w', encoding=\"utf8\") as pre, open('/kaggle/working/eval/pretrain_t5/reference.txt', 'w', encoding=\"utf8\") as re:\n    testloader = DataLoader(test_example, batch_size=1, shuffle=False)\n    length = len(testloader)\n    i = 0\n    print(test_example)\n    for batch in testloader:\n        # Prediction\n        text = batch['code']\n        input_ids = tokenizer(text, return_tensors=\"pt\", max_length=17500, truncation=True, padding=True).input_ids\n        input_ids = input_ids.squeeze(dim=1).to(device)\n        generated_ids = finetune_model.generate(input_ids, max_length=20)\n        result = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n        result = postprocessing(result)\n        pre.write(str(i) + '\\t' + result + \"\\n\")\n\n        # Reference\n        text = batch['docstring']\n        text= postprocessing(text)\n        re.write(str(i) + '\\t' + text + \"\\n\")\n\n        i += 1\n        if ((i % 100) == 0):\n          print(f'{i}/{length}')","metadata":{"execution":{"iopub.status.busy":"2024-05-28T09:30:20.871814Z","iopub.execute_input":"2024-05-28T09:30:20.872241Z","iopub.status.idle":"2024-05-28T09:32:25.193450Z","shell.execute_reply.started":"2024-05-28T09:30:20.872207Z","shell.execute_reply":"2024-05-28T09:32:25.191827Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['id', 'repo', 'path', 'func_name', 'original_string', 'language', 'code', 'code_tokens', 'docstring', 'docstring_tokens', 'sha', 'url'],\n    num_rows: 1261\n})\n100/1261\n200/1261\n300/1261\n400/1261\n500/1261\n600/1261\n700/1261\n800/1261\n900/1261\n1000/1261\n1100/1261\n1200/1261\n","output_type":"stream"}]}]}